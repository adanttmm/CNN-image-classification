{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d8674b5",
   "metadata": {},
   "source": [
    "# Ej CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb25ee3",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2096a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a403d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24028301",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb72ba3",
   "metadata": {},
   "source": [
    "### Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dbd5aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1. / 255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    './dataset/training_set',\n",
    "    target_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6155b32",
   "metadata": {},
   "source": [
    "### Train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27db90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# test is only rescaled to assess performance as in production\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1. / 255\n",
    ")\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    './dataset/test_set',\n",
    "    target_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff2410",
   "metadata": {},
   "source": [
    "## Build CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb90ac6",
   "metadata": {},
   "source": [
    "### Initialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc89349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 17:59:32.524126: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-30 17:59:32.529361: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28568aa2",
   "metadata": {},
   "source": [
    "### step 1 Convolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a799f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters = 32, # feature detectors numbers\n",
    "    kernel_size = 3, # size of the feature detector\n",
    "    activation = 'relu', # introduces non-linearity\n",
    "    input_shape = [64, 64, 3] # only needed in first convolution\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c4bd9",
   "metadata": {},
   "source": [
    "### step 2 Pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30d0f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size = 2, # size of the pooling matrix\n",
    "    strides = 2 # sliding window by\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bb9f3",
   "metadata": {},
   "source": [
    "### Additional convolutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681818cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters = 32, # feature detectors numbers\n",
    "    kernel_size = 3, # size of the feature detector\n",
    "    activation = 'relu' # introduces non-linearity\n",
    "))\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size = 2, # size of the pooling matrix\n",
    "    strides = 2 # sliding window by\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190482bb",
   "metadata": {},
   "source": [
    "### step 3 flattening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f679b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9117d42",
   "metadata": {},
   "source": [
    "### step 4 fully connected layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3733889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef8d68",
   "metadata": {},
   "source": [
    "### step 5 output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff52c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d19df7",
   "metadata": {},
   "source": [
    "## Train CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d469c2",
   "metadata": {},
   "source": [
    "### compile CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cda3ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e3912f",
   "metadata": {},
   "source": [
    "### training on train dataset and evaluating on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82553efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 116s 459ms/step - loss: 0.6696 - accuracy: 0.5954 - val_loss: 0.6249 - val_accuracy: 0.6565\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 106s 424ms/step - loss: 0.6154 - accuracy: 0.6655 - val_loss: 0.5589 - val_accuracy: 0.7170\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 101s 406ms/step - loss: 0.5660 - accuracy: 0.7045 - val_loss: 0.5344 - val_accuracy: 0.7435\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 105s 419ms/step - loss: 0.5306 - accuracy: 0.7385 - val_loss: 0.4934 - val_accuracy: 0.7595\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 86s 346ms/step - loss: 0.5007 - accuracy: 0.7509 - val_loss: 0.4918 - val_accuracy: 0.7590\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 101s 406ms/step - loss: 0.4776 - accuracy: 0.7700 - val_loss: 0.4687 - val_accuracy: 0.7825\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 97s 388ms/step - loss: 0.4585 - accuracy: 0.7809 - val_loss: 0.4727 - val_accuracy: 0.7800\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 97s 390ms/step - loss: 0.4389 - accuracy: 0.7969 - val_loss: 0.4707 - val_accuracy: 0.7765\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 95s 382ms/step - loss: 0.4274 - accuracy: 0.8015 - val_loss: 0.4408 - val_accuracy: 0.7960\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 97s 390ms/step - loss: 0.4185 - accuracy: 0.8065 - val_loss: 0.4651 - val_accuracy: 0.7845\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 95s 382ms/step - loss: 0.4005 - accuracy: 0.8156 - val_loss: 0.4618 - val_accuracy: 0.7865\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 89s 355ms/step - loss: 0.3834 - accuracy: 0.8264 - val_loss: 0.4816 - val_accuracy: 0.7835\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 87s 349ms/step - loss: 0.3691 - accuracy: 0.8356 - val_loss: 0.4519 - val_accuracy: 0.8025\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 87s 347ms/step - loss: 0.3603 - accuracy: 0.8385 - val_loss: 0.4497 - val_accuracy: 0.8060\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 89s 357ms/step - loss: 0.3538 - accuracy: 0.8395 - val_loss: 0.4701 - val_accuracy: 0.7975\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 89s 358ms/step - loss: 0.3357 - accuracy: 0.8504 - val_loss: 0.5179 - val_accuracy: 0.7755\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 89s 355ms/step - loss: 0.3122 - accuracy: 0.8630 - val_loss: 0.4500 - val_accuracy: 0.8135\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.3044 - accuracy: 0.8709 - val_loss: 0.5493 - val_accuracy: 0.7645\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 88s 353ms/step - loss: 0.2952 - accuracy: 0.8770 - val_loss: 0.4757 - val_accuracy: 0.8150\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 86s 343ms/step - loss: 0.2724 - accuracy: 0.8825 - val_loss: 0.4633 - val_accuracy: 0.8165\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 85s 339ms/step - loss: 0.2625 - accuracy: 0.8921 - val_loss: 0.5256 - val_accuracy: 0.8060\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 56592s 227s/step - loss: 0.2547 - accuracy: 0.8888 - val_loss: 0.5009 - val_accuracy: 0.8030\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 111s 444ms/step - loss: 0.2305 - accuracy: 0.9084 - val_loss: 0.5156 - val_accuracy: 0.8210\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 91s 364ms/step - loss: 0.2242 - accuracy: 0.9097 - val_loss: 0.5350 - val_accuracy: 0.8185\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.2069 - accuracy: 0.9162 - val_loss: 0.5458 - val_accuracy: 0.8105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e91aa57b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = train_dataset, validation_data = test_dataset, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c8b6a",
   "metadata": {},
   "source": [
    "## Making single prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('./dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "# preprocess image to fit the network\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "training_dataset.class_indices\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c43fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3572e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f02ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
